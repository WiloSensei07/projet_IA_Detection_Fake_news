{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commandes de base\n",
    "- python.exe -m pip install --upgrade pip\n",
    "- python3 -m venv .venv\n",
    "- .\\.venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commande 1: Mise à jour de `pip`\n",
    "```sh\n",
    "python.exe -m pip install --upgrade pip\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- **Sécurité** : Les mises à jour incluent souvent des correctifs de sécurité.\n",
    "- **Compatibilité** : Les nouvelles versions sont souvent mieux compatibles avec les nouvelles versions de Python et les nouveaux paquets.\n",
    "- **Fonctionnalités et Stabilité** : Les mises à jour incluent de nouvelles fonctionnalités et corrections de bugs.\n",
    "\n",
    "### Commande 2: Création d'un environnement virtuel\n",
    "```sh\n",
    "python3 -m venv .venv\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- **Isolation** : Les environnements virtuels permettent d'isoler les dépendances des projets. Cela évite les conflits de versions entre les projets.\n",
    "- **Facilité de gestion** : Chaque projet peut avoir ses propres dépendances spécifiques, indépendamment des autres projets.\n",
    "\n",
    "### Commande 3: Activation de l'environnement virtuel\n",
    "```sh\n",
    ".\\.venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- **Utilisation des dépendances spécifiques** : Une fois activé, l'environnement virtuel utilise les versions de paquets installées dans cet environnement, plutôt que celles installées globalement.\n",
    "- **Contexte de développement propre** : Permet de travailler dans un contexte de développement propre et spécifique au projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecte et Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df=pd.read_csv(r\"dataset/transfers.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des valeurs nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# reload the dataset\n",
    "data = df.copy()\n",
    "\n",
    "# Handle missing values\n",
    "data.ffill(inplace=True)\n",
    "data.is_free = data.is_free.apply(lambda x: True if x=='TRUE' else False)\n",
    "#data.is_loan = data.is_loan.apply(lambda x: True if x=='TRUE' else False)\n",
    "#data.is_loan_end = data.is_loan_end.apply(lambda x: True if x=='TRUE' else False)\n",
    "data.is_retired = data.is_retired.apply(lambda x: True if x=='TRUE' else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les valeurs manquantes\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choix des variables pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns of interest\n",
    "columns_of_interest = [\n",
    "    'league', 'season', 'team_name', 'team_country', 'player_name', 'player_age',\n",
    "    'counter_team_name', 'counter_team_country', 'transfer_fee_amnt', 'market_val_amnt', 'is_free', 'is_retired'\n",
    "]\n",
    "\n",
    "# Select only the columns of interest\n",
    "data = data[columns_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodage des varaibles pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des paires afin de réduire le dataset\n",
    "\n",
    "Car nous avons rencontré un temps trop élévé de compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairs of data with a reduced dataset\n",
    "def create_random_pairs(data, num_pairs=10000):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    num_samples = len(data)\n",
    "    \n",
    "    for _ in range(num_pairs):\n",
    "        i, j = np.random.choice(num_samples, 2, replace=False)\n",
    "        pairs.append((data.iloc[i], data.iloc[j]))\n",
    "        # Label 1 for similar (same player_name and season), 0 for non-similar (different player_name or season)\n",
    "        labels.append(1 if data.iloc[i]['team_name'] == data.iloc[j]['team_name'] else 0)\n",
    "    \n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "# Reduce the dataset size for demonstration\n",
    "reduced_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "pairs, labels = create_random_pairs(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation du dataset pour entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the pairs into training and testing sets\n",
    "pairs_train, pairs_test, labels_train, labels_test = train_test_split(pairs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the input for the model\n",
    "def prepare_input(pairs):\n",
    "    X = []\n",
    "    for pair in pairs:\n",
    "        X.append(np.concatenate((pair[0], pair[1])))\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amélioration des balances entre les classes (paires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the classes using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = prepare_input(pairs_train), labels_train\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement et évaluation du modèle : RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prepare_input(pairs_test)\n",
    "\n",
    "# Train a RandomForest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, labels_test)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(labels_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(labels_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(labels_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparons ce qui sera rentré par l'utilisateur avec ce qui sera appris du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare user input with the model and identify likely false fields\n",
    "def compare_user_input(user_input, data, model, label_encoders):\n",
    "    # Encode the user input\n",
    "    user_input_encoded = {}\n",
    "    for column in columns_of_interest:\n",
    "        if column in label_encoders:\n",
    "            # Add new values to the encoder\n",
    "            if user_input.get(column, '') not in label_encoders[column].classes_:\n",
    "                label_encoders[column].classes_ = np.append(label_encoders[column].classes_, user_input.get(column, ''))\n",
    "            user_input_encoded[column] = label_encoders[column].transform([user_input.get(column, '')])[0]\n",
    "        else:\n",
    "            user_input_encoded[column] = user_input.get(column, 0)\n",
    "\n",
    "    user_input_array = np.array([user_input_encoded.get(col, 0) for col in columns_of_interest])\n",
    "\n",
    "    # Create pairs with examples from the dataset\n",
    "    pairs = []\n",
    "    for i in range(len(data)):\n",
    "        data_row = data.iloc[i].values\n",
    "        pairs.append(np.concatenate((user_input_array, data_row)))\n",
    "    \n",
    "    pairs = np.array(pairs)\n",
    "    \n",
    "    # Predict similarity probabilities\n",
    "    probabilities = model.predict_proba(pairs)[:, 1]  # Probability of being similar (label 1)\n",
    "    \n",
    "    # Calculate the overall truth percentage\n",
    "    max_probability = np.max(probabilities)\n",
    "    truth_percentage = max_probability * 100  # Convert to percentage\n",
    "\n",
    "    # Identify likely false fields\n",
    "    likely_false_fields = []\n",
    "    for column in columns_of_interest:\n",
    "        modified_input = user_input_array.copy()\n",
    "        original_value = modified_input[columns_of_interest.index(column)]\n",
    "        for i in range(len(data)):\n",
    "            data_row = data.iloc[i].values\n",
    "            # Change the value of the column to match the dataset and see if probability increases\n",
    "            modified_input[columns_of_interest.index(column)] = data_row[columns_of_interest.index(column)]\n",
    "            modified_pairs = [np.concatenate((modified_input, data_row))]\n",
    "            modified_probability = model.predict_proba(modified_pairs)[:, 1]\n",
    "            if np.max(modified_probability) > max_probability:\n",
    "                likely_false_fields.append(column)\n",
    "                break\n",
    "            modified_input[columns_of_interest.index(column)] = original_value  # Reset to original value\n",
    "\n",
    "    return truth_percentage, likely_false_fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user input\n",
    "user_input = {\n",
    "    'league': 'GB1',\n",
    "    'season': 2009,\n",
    "    'team_name': 'Abdoulaye-ismael KOULIBALY',\n",
    "    'team_country': 'Lindsay CENESCART MARSEILLE',\n",
    "    'player_name': 'TCHATHCOU SINKAM Wilfried',\n",
    "    'player_age': 24.0,\n",
    "    'player_nation': 'Joan Cindy MIKONGO OUAMBO',\n",
    "    'counter_team_name': 'Guillaume WALES',\n",
    "    'counter_team_country': 'Gilchrist DONHISSOU',\n",
    "    'transfer_fee_amnt': 94000000.0,\n",
    "    'market_val_amnt': 45000000.0,\n",
    "    'is_free': False,\n",
    "    'is_retired': False\n",
    "}\n",
    "\n",
    "# Compare user input with the model\n",
    "truth_percentage, likely_false_fields = compare_user_input(user_input, reduced_data, model, label_encoders)\n",
    "print(\"Truth Percentage:\", truth_percentage)\n",
    "print(\"Likely False Fields:\", likely_false_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement et Evaluation de la Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entrainement d'un modèle de régression logistique\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluation du modèle\n",
    "logistic_accuracy = logistic_model.score(X_test, labels_test)\n",
    "print(\"Logistic Model Accuracy:\", logistic_accuracy)\n",
    "\n",
    "# Prédictions\n",
    "logistic_y_pred = logistic_model.predict(X_test)\n",
    "logistic_y_prob = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Impression des métriques d'évaluation\n",
    "print(\"Logistic Accuracy:\", accuracy_score(labels_test, logistic_y_pred))\n",
    "print(\"Logistic Classification Report:\\n\", classification_report(labels_test, logistic_y_pred))\n",
    "print(\"Logistic ROC AUC Score:\", roc_auc_score(labels_test, logistic_y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement et Evaluation de la Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Entrainement d'un modèle de régression linéaire\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluation du modèle\n",
    "y_pred_continuous = linear_model.predict(X_test)\n",
    "\n",
    "# Convertir les prédictions continues en classifications binaires\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "y_pred_binary = binarizer.fit_transform(y_pred_continuous.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Calculer les probabilités pour ROC AUC Score\n",
    "y_prob = y_pred_continuous\n",
    "\n",
    "# Impression des métriques d'évaluation\n",
    "print(\"Linear Regression Model Accuracy:\", accuracy_score(labels_test, y_pred_binary))\n",
    "print(\"Linear Regression Classification Report:\\n\", classification_report(labels_test, y_pred_binary))\n",
    "print(\"Linear Regression ROC AUC Score:\", roc_auc_score(labels_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement et Evaluation du Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Entrainement d'un modèle Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluation du modèle\n",
    "gb_accuracy = gb_model.score(X_test, labels_test)\n",
    "print(\"Gradient Boosting Model Accuracy:\", gb_accuracy)\n",
    "\n",
    "# Prédictions\n",
    "gb_y_pred = gb_model.predict(X_test)\n",
    "gb_y_prob = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Impression des métriques d'évaluation\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(labels_test, gb_y_pred))\n",
    "print(\"Gradient Boosting Classification Report:\\n\", classification_report(labels_test, gb_y_pred))\n",
    "print(\"Gradient Boosting ROC AUC Score:\", roc_auc_score(labels_test, gb_y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement et Evaluation du SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Entrainement d'un modèle SVM\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "svm_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluation du modèle\n",
    "svm_accuracy = svm_model.score(X_test, labels_test)\n",
    "print(\"SVM Model Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Prédictions\n",
    "svm_y_pred = svm_model.predict(X_test)\n",
    "svm_y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Impression des métriques d'évaluation\n",
    "print(\"SVM Accuracy:\", accuracy_score(labels_test, svm_y_pred))\n",
    "print(\"SVM Classification Report:\\n\", classification_report(labels_test, svm_y_pred))\n",
    "print(\"SVM ROC AUC Score:\", roc_auc_score(labels_test, svm_y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion générale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au vue de ces différents modèles, nous pouvons conserver uniquement le modelèle de RandomForest qui présente de meilleurs caractériques par rapports aux autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrement et nouveau test en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du meilleur modèle\n",
    "import joblib\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "joblib.dump(model, 'model/best_model.pkl')\n",
    "\n",
    "# Sauvegarder les encoders\n",
    "joblib.dump(label_encoders, 'model/label_encoders.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Charger les données pour créer les encoders\n",
    "data = df.copy()\n",
    "\n",
    "# Colonnes d'intérêt\n",
    "columns_of_interest = [\n",
    "    'league', 'season', 'team_name', 'team_country', 'player_name', 'player_age',\n",
    "    'counter_team_name', 'counter_team_country', 'transfer_fee_amnt', 'market_val_amnt', 'is_free', 'is_retired'\n",
    "]\n",
    "\n",
    "# Créer les encoders pour les colonnes catégorielles\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Charger le modèle\n",
    "model = joblib.load('model/best_model.pkl')\n",
    "\n",
    "# Fonction de comparaison des entrées utilisateur\n",
    "def compare_user_input(user_input, data, model, label_encoders):\n",
    "    # Encode the user input\n",
    "    user_input_encoded = {}\n",
    "    for column in columns_of_interest:\n",
    "        if column in label_encoders:\n",
    "            if user_input.get(column, '') not in label_encoders[column].classes_:\n",
    "                label_encoders[column].classes_ = np.append(label_encoders[column].classes_, user_input.get(column, ''))\n",
    "            user_input_encoded[column] = label_encoders[column].transform([user_input.get(column, '')])[0]\n",
    "        else:\n",
    "            user_input_encoded[column] = user_input.get(column, 0)\n",
    "\n",
    "    user_input_array = np.array([user_input_encoded.get(col, 0) for col in columns_of_interest])\n",
    "\n",
    "    # Create pairs with examples from the dataset\n",
    "    pairs = []\n",
    "    for i in range(len(data)):\n",
    "        data_row = data.iloc[i].values\n",
    "        pairs.append(np.concatenate((user_input_array, data_row)))\n",
    "\n",
    "    pairs = np.array(pairs)\n",
    "\n",
    "    # Predict similarity probabilities\n",
    "    probabilities = model.predict_proba(pairs)[:, 1]  # Probability of being similar (label 1)\n",
    "\n",
    "    # Calculate the overall truth percentage\n",
    "    max_probability = np.max(probabilities)\n",
    "    truth_percentage = max_probability * 100  # Convert to percentage\n",
    "\n",
    "    # Identify likely false fields\n",
    "    likely_false_fields = []\n",
    "    for column in columns_of_interest:\n",
    "        modified_input = user_input_array.copy()\n",
    "        original_value = modified_input[columns_of_interest.index(column)]\n",
    "        for i in range(len(data)):\n",
    "            data_row = data.iloc[i].values\n",
    "            # Change the value of the column to match the dataset and see if probability increases\n",
    "            modified_input[columns_of_interest.index(column)] = data_row[columns_of_interest.index(column)]\n",
    "            modified_pairs = [np.concatenate((modified_input, data_row))]\n",
    "            modified_probability = model.predict_proba(modified_pairs)[:, 1]\n",
    "            if np.max(modified_probability) > max_probability:\n",
    "                likely_false_fields.append(column)\n",
    "                break\n",
    "            modified_input[columns_of_interest.index(column)] = original_value  # Reset to original value\n",
    "\n",
    "    return truth_percentage, likely_false_fields\n",
    "\n",
    "# Example user input\n",
    "user_input = {\n",
    "    'league': 'GB1',\n",
    "    'season': 2009,\n",
    "    'team_name': 'Manchester United',\n",
    "    'team_country': 'England',\n",
    "    'player_name': 'Cristiano Ronaldo',\n",
    "    'player_age': 24.0,\n",
    "    'player_nation': 'Portugal',\n",
    "    'counter_team_name': 'Real Madrid',\n",
    "    'counter_team_country': 'Spain',\n",
    "    'transfer_fee_amnt': 94000000.0,\n",
    "    'market_val_amnt': 45000000.0,\n",
    "    'is_free': False,\n",
    "    'is_retired': False\n",
    "}\n",
    "\n",
    "# Comparer l'entrée utilisateur avec le modèle\n",
    "truth_percentage, likely_false_fields = compare_user_input(user_input, reduced_data, model, label_encoders)\n",
    "print(\"Truth Percentage:\", truth_percentage)\n",
    "print(\"Likely False Fields:\", likely_false_fields)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
